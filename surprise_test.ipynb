{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPICVUvnbxitPdaRQPM+vtk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DotSlash-A/Pytorch/blob/main/surprise_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDwwCUeX130x"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q.** The diagram below shows a neural network used for classification problem. The network contains two hidden layers and one output layer. The input to the network is a column vector $\\textbf{x} \\in \\mathbb{R}^{3}$. The first layer contains 3 neurons, the second hidden layer contains 3 neurons and the output layer contains 3 neurons. Each neuron in the $l^{th}$ layer is connected to all the neurons in the $(l + 1)^{th}$ layer. Each neuron has a bias connected to it. (not shown in the figure)\n",
        "\n",
        "![](https://backend.seek.onlinedegree.iitm.ac.in/22t3_cs3004/assets/img/W3GA1.png)\n",
        "\n",
        "All the neurons in the hidden layer use Sigmoid activation function and the neurons in the output layer uses Softmax function. Assume that the network uses cross entropy loss (use natural log)\n"
      ],
      "metadata": {
        "id": "zkWQP-d52Blo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q. 1:** How many learnable parameters?"
      ],
      "metadata": {
        "id": "tNcP7FCk2ToX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A:** $3 \\times 3 + 3 \\times 3 + 3 \\times 3 + 3 + 3 + 3 = 9 + 9 + 9 + 9 = 36$"
      ],
      "metadata": {
        "id": "gwfews1i2pXD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w1 = torch.tensor([\n",
        "  [0.5488135, 0.71518937, 0.60276338],\n",
        "  [0.54488318, 0.4236548, 0.64589411],\n",
        "  [0.43758721, 0.891773, 0.96366276]\n",
        "], dtype = torch.float64, requires_grad = True)\n",
        "\n",
        "w2 = torch.tensor([\n",
        "  [0.56804456, 0.92559664, 0.07103606],\n",
        "  [0.0871293, 0.0202184, 0.83261985],\n",
        "  [0.77815675, 0.87001215, 0.97861834]\n",
        "], dtype = torch.float64, requires_grad = True)\n",
        "\n",
        "w3 = torch.tensor([\n",
        "  [0.11827443, 0.63992102, 0.14335329],\n",
        "  [0.94466892, 0.52184832, 0.41466194],\n",
        "  [0.26455561, 0.77423369, 0.45615033]\n",
        "], dtype = torch.float64, requires_grad = True)"
      ],
      "metadata": {
        "id": "B17g-_OS17r5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b1 = torch.tensor([0.38344152, 0.79172504, 0.52889492], dtype = torch.float64, requires_grad = True)\n",
        "b2 = torch.tensor([0.79915856, 0.46147936, 0.78052918], dtype = torch.float64, requires_grad = True)\n",
        "b3 = torch.tensor([0.56843395, 0.0187898, 0.6176355], dtype = torch.float64, requires_grad = True)"
      ],
      "metadata": {
        "id": "1LY9zylj257R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([1.0, 0.0, 1.0], dtype = torch.float64)\n",
        "y = torch.tensor([0, 0, 1], dtype = torch.float64)"
      ],
      "metadata": {
        "id": "R2sbiWdG271h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q. 2:** What is the sum of elements of output $\\textbf{a}_{1}?$"
      ],
      "metadata": {
        "id": "EZmBKvVP2t7d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a1=w1@x+b1"
      ],
      "metadata": {
        "id": "bdWGWH0X29ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a1.sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QyMn0Hue3tGs",
        "outputId": "82e5e985-2d57-40fb-bb2d-6d6a188b6780"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(5.4477, dtype=torch.float64, grad_fn=<SumBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a1 = w1 @ x + b1\n",
        "print(f'Sum of elements of a1: {torch.sum(a1):.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEa6aylh3t6W",
        "outputId": "2e94640c-781c-4aea-df04-73c75ded5e7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sum of elements of a1: 5.45\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q. 3:** What is the sum of elements of output $\\textbf{h}_{1}?$"
      ],
      "metadata": {
        "id": "RytKoCZj4Cz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "h1=torch.sigmoid(a1)"
      ],
      "metadata": {
        "id": "i2KZJkXn35yo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "h1.sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbTLzrje4dsR",
        "outputId": "7753fc03-5ebe-426e-a006-4368ca9f891c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.5750, dtype=torch.float64, grad_fn=<SumBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hidden layers use sigmoid as activation\n",
        "h1 = torch.sigmoid(a1)\n",
        "print(f'Sum of elements of h1: {torch.sum(h1):.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa0od0M14fDg",
        "outputId": "55b09957-1986-410b-c35b-034af439e3f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sum of elements of h1: 2.57\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q:** The sum of elements of $[\\textbf{a}_{2}, \\textbf{h}_{2}, \\textbf{a}_{3}]$, respectively are $[6.4, 2.63, 4.87]$. What is the loss value?"
      ],
      "metadata": {
        "id": "Nw2Mdcjc4ndT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a2=w2@h1+b2"
      ],
      "metadata": {
        "id": "Ia5dXK--4m5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRcX0P6j5GVK",
        "outputId": "2ce12254-fa24-42ad-811b-e24578ecd460"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2.1421, 1.2780, 3.0400], dtype=torch.float64, grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "h2=torch.sigmoid(a2)"
      ],
      "metadata": {
        "id": "7HS3-MnI5MQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a3=w3@h2+b3"
      ],
      "metadata": {
        "id": "2dfBjkRF5QNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = torch.nn.functional.cross_entropy(a3.view(1, -1), torch.argmax(y).view(1))"
      ],
      "metadata": {
        "id": "TwCX9aYJ5W80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IpnYu-XK5eSU",
        "outputId": "92248cf7-3b69-4b54-8f2b-67e5dfb27910"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.8564, dtype=torch.float64, grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "array_given_in_question = []\n",
        "\n",
        "a2 = w2 @ h1 + b2\n",
        "array_given_in_question.append(torch.sum(a2).item())\n",
        "\n",
        "h2 = torch.sigmoid(a2)\n",
        "array_given_in_question.append(torch.sum(h2).item())\n",
        "\n",
        "a3 = w3 @ h2 + b3\n",
        "array_given_in_question.append(torch.sum(a3).item())\n",
        "\n",
        "# Loss function used here is cross entropy, it applies the softmax on the output layer\n",
        "loss = torch.nn.functional.cross_entropy(a3.view(1, -1), torch.argmax(y).view(1))\n",
        "print(f'Array given in the question: {array_given_in_question}')\n",
        "print(f'Loss: {loss.item():.2f}', end = '')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4DHz6ua5pb3",
        "outputId": "1071d278-e70e-4ea4-a351-70d5063773cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Array given in the question: [6.460166777022406, 2.6313930964242145, 4.874920995857018]\n",
            "Loss: 0.86"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q:** What is the vector that corresponds to $\\nabla_{a_{3}}\\mathscr{L}(\\theta)$"
      ],
      "metadata": {
        "id": "MVUjeNbi6CD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.autograd.grad(loss,a3,retain_graph=True)[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEulaBKS6BcQ",
        "outputId": "2198832e-8508-4003-afe9-9956dea257c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.2369,  0.3384, -0.5753], dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d_loss_d_a3 = torch.autograd.grad(loss, a3, retain_graph = True)[0]\n",
        "display(d_loss_d_a3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "z4IJJYhK7W1p",
        "outputId": "03459600-7d3a-4ba7-d063-88cd9288ba03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tensor([ 0.2369,  0.3384, -0.5753], dtype=torch.float64)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q:** We know that after computing gradients, we update the values of $\\textbf{b}_{2}$ by subtracting its gradient, as shown below \\\\\n",
        "$$\n",
        "\\textbf{b}_{2} - \\eta\\nabla_{b_{2}}\\mathscr{L}(\\theta)\n",
        "$$\n",
        "Which of the following is gradient vector of $\\textbf{b}_{2}$"
      ],
      "metadata": {
        "id": "bN-0Txvg7ko_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.autograd.grad(loss,b2,retain_graph=True)[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVffg5E77oun",
        "outputId": "060693aa-b1f8-42ef-d895-7899b53fe9bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0184, -0.0200, -0.0038], dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q:** Update all the parameters with calculated gradients. Forward propagate through the network. What is the new loss value? (Take $\\eta = 1$)"
      ],
      "metadata": {
        "id": "X4HbBq2k8DiW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dw1 = torch.autograd.grad(loss, w1, retain_graph = True)[0]\n",
        "dw2 =torch.autograd.grad(loss,w2,retain_graph = True)[0]\n",
        "dw3 =torch.autograd.grad(loss,w3,retain_graph = True)[0]\n",
        "db1=torch.autograd.grad(loss,b1,retain_graph = True)[0]\n",
        "db2=torch.autograd.grad(loss,b2,retain_graph = True)[0]\n",
        "db3=torch.autograd.grad(loss,b3,retain_graph = True)[0]"
      ],
      "metadata": {
        "id": "SJMxE5_Y7yN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\theta_{\\text{new}} = \\theta_{\\text{old}} - \\eta \\nabla\\mathscr{L}(\\theta)$"
      ],
      "metadata": {
        "id": "V2-Nsz4yBQkU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w1=w1-dw1\n",
        "w2=w2-dw2\n",
        "w3=w3-dw3\n",
        "b1=b1-db1\n",
        "b2=b2-db2\n",
        "b3=b3-db3"
      ],
      "metadata": {
        "id": "QRVyuG8yBU3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a1=w1@x+b1\n",
        "h1 = torch.sigmoid(a1)\n",
        "a2=w2@h1+b2\n",
        "h2= torch.sigmoid(a2)\n",
        "a3=w3@h2+b3\n"
      ],
      "metadata": {
        "id": "Q78GWRdTBZWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = torch.nn.functional.cross_entropy(a3.view(1, -1), torch.argmax(y).view(1))"
      ],
      "metadata": {
        "id": "xZkh5JE1CDJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "498rBLZsCEgo",
        "outputId": "ac0ac822-e028-4089-8564-48d55c4a0f0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.0725, dtype=torch.float64, grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lr = 1.0\n",
        "\n",
        "# dw1 = torch.autograd.grad(loss, w1, retain_graph = True)[0]\n",
        "# dw2 = torch.autograd.grad(loss, w2, retain_graph = True)[0]\n",
        "# dw3 = torch.autograd.grad(loss, w3, retain_graph = True)[0]\n",
        "# db1 = torch.autograd.grad(loss, b1, retain_graph = True)[0]\n",
        "# db2 = torch.autograd.grad(loss, b2, retain_graph = True)[0]\n",
        "# db3 = torch.autograd.grad(loss, b3, retain_graph = True)[0]\n",
        "\n",
        "# w1 = w1 - lr * dw1\n",
        "# w2 = w2 - lr * dw2\n",
        "# w3 = w3 - lr * dw3\n",
        "# b1 = b1 - lr * db1\n",
        "# b2 = b2 - lr * db2\n",
        "# b3 = b3 - lr * db3\n",
        "\n",
        "# a1 = w1 @ x + b1\n",
        "# h1 = torch.sigmoid(a1)\n",
        "\n",
        "# a2 = w2 @ h1 + b2\n",
        "# h2 = torch.sigmoid(a2)\n",
        "\n",
        "# a3 = w3 @ h2 + b3\n",
        "# # h3 = torch.nn.functional.softmax(a3, dim = 0)\n",
        "\n",
        "# new_loss = torch.nn.functional.cross_entropy(a3.view(1, -1), torch.argmax(y).view(1))\n",
        "\n",
        "# display(new_loss.item())"
      ],
      "metadata": {
        "id": "MceV39zRCFBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8ite99XHC1DE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# w1 = torch.tensor([\n",
        "#   [0.5488135, 0.71518937, 0.60276338],\n",
        "#   [0.54488318, 0.4236548, 0.64589411],\n",
        "#   [0.43758721, 0.891773, 0.96366276]\n",
        "# ], dtype = torch.float64, requires_grad = True)\n",
        "# w2 = torch.tensor([\n",
        "#   [0.56804456, 0.92559664, 0.07103606],\n",
        "#   [0.0871293, 0.0202184, 0.83261985],\n",
        "#   [0.77815675, 0.87001215, 0.97861834]\n",
        "# ], dtype = torch.float64, requires_grad = True)\n",
        "\n",
        "# w3 = torch.tensor([\n",
        "#   [0.11827443, 0.63992102, 0.14335329],\n",
        "#   [0.94466892, 0.52184832, 0.41466194],\n",
        "#   [0.26455561, 0.77423369, 0.45615033]\n",
        "# ], dtype = torch.float64, requires_grad = True)"
      ],
      "metadata": {
        "id": "wDmiOX2lC1QG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# b1 = torch.tensor([0.38344152, 0.79172504, 0.52889492], dtype = torch.float64, requires_grad = True)\n",
        "# b2 = torch.tensor([0.79915856, 0.46147936, 0.78052918], dtype = torch.float64, requires_grad = True)\n",
        "# b3 = torch.tensor([0.56843395, 0.0187898, 0.6176355], dtype = torch.float64, requires_grad = True)"
      ],
      "metadata": {
        "id": "HCpJ-EEgC5SH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x = torch.tensor([1.0, 0.0, 1.0], dtype = torch.float64)\n",
        "# y = torch.tensor([0, 0, 1], dtype = torch.float64)"
      ],
      "metadata": {
        "id": "8NzFR38AC6q5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(NeuralNetwork, self).__init__()\n",
        "    self.fc1 = torch.nn.Linear(3, 3)\n",
        "    self.fc2 = torch.nn.Linear(3, 3)\n",
        "    self.fc3 = torch.nn.Linear(3, 3)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = torch.sigmoid(self.fc1(x))\n",
        "    x = torch.sigmoid(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "3G3a76miCVMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn = NeuralNetwork()\n",
        "\n",
        "nn.fc1.weight.data = w1\n",
        "nn.fc1.bias.data = b1\n",
        "\n",
        "nn.fc2.weight.data = w2\n",
        "nn.fc2.bias.data = b2\n",
        "\n",
        "nn.fc3.weight.data = w3\n",
        "nn.fc3.bias.data = b3"
      ],
      "metadata": {
        "id": "PyRS4-dLCjvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(nn.parameters(), lr = 1.0)"
      ],
      "metadata": {
        "id": "D7wW8-q4Cmzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1):\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  preds = nn(x)\n",
        "  loss = loss_fn(preds.view(1, -1), torch.argmax(y).view(1))\n",
        "\n",
        "  loss.backward() #replaced cell 22 with this\n",
        "  optimizer.step()#replaced cell 23 with this"
      ],
      "metadata": {
        "id": "cJC8u8qcCoPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "TLc2O6bfCqQ-",
        "outputId": "a0b608c3-d92a-4857-d071-20f82937b26e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0.0725333007370964"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tBdr9C9jC-bb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}